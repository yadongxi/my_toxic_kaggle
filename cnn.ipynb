{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "2c09a73b-225c-4cb3-b6e5-8360cfe27bef",
    "_uuid": "461c683283c62f10be20c61a56dfdadc4e38c392"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Flatten, Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Don't Show Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "8ff1b6f1-2e72-4eec-954e-017ced59dce5",
    "_uuid": "2baebccd3836f4bda9b03ac3ea85b2fd511e73c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 8)\n",
      "(226998, 2)\n"
     ]
    }
   ],
   "source": [
    "# read in the data\n",
    "\n",
    "#df_train = pd.read_csv('train.csv.zip')\n",
    "#df_test = pd.read_csv('test.csv.zip')\n",
    "\n",
    "df_train = pd.read_csv('input/train.csv')\n",
    "df_test = pd.read_csv('input/test.csv')\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "20b9b3aa-110b-4c59-9970-1732e87017fa",
    "_uuid": "c4392f8fc9f88c5daef65e66d233b72093de8a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322849, 8)\n"
     ]
    }
   ],
   "source": [
    "# combine the train and test sets for encoding and padding\n",
    "\n",
    "train_len = len(df_train)\n",
    "df_combined =  pd.concat(objs=[df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(df_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "0b23211a-fb43-4fe3-a855-86317a7937ce",
    "_uuid": "f5b22283efd3faef27316ea3c65867f688427817"
   },
   "outputs": [],
   "source": [
    "# define text data\n",
    "docs_combined = df_combined['comment_text'].astype(str)\n",
    "\n",
    "# initialize the tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs_combined)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the text data\n",
    "encoded_docs = t.texts_to_sequences(docs_combined)\n",
    "\n",
    "# pad the vectors to create uniform length\n",
    "padded_docs_combined = pad_sequences(encoded_docs, maxlen=500, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378178"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "210f64cb-5e3c-4e08-90ad-dbbb23b90df8",
    "_uuid": "410ce48a5a51e8ecb2e75bf9ecf9c2473e0b4cdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 500)\n",
      "(226998, 500)\n"
     ]
    }
   ],
   "source": [
    "# seperate the train and test sets\n",
    "\n",
    "df_train_padded = padded_docs_combined[:train_len]\n",
    "df_test_padded = padded_docs_combined[train_len:]\n",
    "\n",
    "print(df_train_padded.shape)\n",
    "print(df_test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "c1ddb595-c6dd-4a10-812e-9528f0489fab",
    "_uuid": "3c7fca0da64ff99701ff9d30223e598046c1ce05",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.840B.300d.txt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4172a511f463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.840B.300d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.840B.300d.txt'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# load the glove840B embedding into memory after downloading and unzippping\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('glove.840B.300d.txt')\n",
    "\n",
    "for line in f:\n",
    "    # Note: use split(' ') instead of split() if you get an error.\n",
    "\tvalues = line.split(' ')\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "9d242e26-e9a0-4ae3-a47c-97a41e638343",
    "_uuid": "086e9e087dc425b4ff79564680bb47ac55569776",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_train_padded\n",
    "X_test = df_test_padded\n",
    "\n",
    "# target columns\n",
    "y_toxic = df_train['toxic']\n",
    "y_severe_toxic = df_train['severe_toxic']\n",
    "y_obscene = df_train['obscene']\n",
    "y_threat = df_train['threat']\n",
    "y_insult = df_train['insult']\n",
    "y_identity_hate = df_train['identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "6d9218a9-de6e-4df9-b667-c394c2bd8495",
    "_uuid": "8d94984e6a3508f9f781007c17be6ad950fa501e",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bbe218777aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# cnn model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     e = Embedding(vocab_size, 300, weights=[embedding_matrix], \n\u001b[0m\u001b[1;32m     20\u001b[0m                   input_length=500, trainable=False)\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# create a list of the target columns\n",
    "target_cols = [y_toxic,y_severe_toxic,y_obscene,y_threat,y_insult,y_identity_hate]\n",
    "\n",
    "preds = []\n",
    "\n",
    "for col in target_cols:\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    # set the value of y\n",
    "    y = col\n",
    "    \n",
    "    # create a stratified split\n",
    "    X_train, X_eval, y_train ,y_eval = train_test_split(X, y,test_size=0.25,shuffle=True,\n",
    "                                                    random_state=5,stratify=y)\n",
    "\n",
    "    # cnn model\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], \n",
    "                  input_length=500, trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    # compile the model\n",
    "    Adam_opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=Adam_opt, loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "    save_best = ModelCheckpoint('toxic.hdf', save_best_only=True, \n",
    "                               monitor='val_loss', mode='min')\n",
    "\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_eval, y_eval),\n",
    "                        epochs=100, verbose=1,callbacks=[early_stopping,save_best])\n",
    "\n",
    "    \n",
    "    # make a prediction on y (target column)\n",
    "    model.load_weights(filepath = 'toxic.hdf')\n",
    "    predictions = model.predict(X_test)\n",
    "    y_preds = predictions[:,0]\n",
    "    \n",
    "    # append the prediction to a python list\n",
    "    preds.append(y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "1cce236f-c2a8-4a49-af46-e3f757d2a96b",
    "_uuid": "34d25b09fe313b0d94aabb0849929512e5c7f803",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1279dbb4aab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df_results = pd.DataFrame({'id':df_test.id,\n\u001b[0;32m----> 2\u001b[0;31m                             \u001b[0;34m'toxic'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                            \u001b[0;34m'severe_toxic'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                            \u001b[0;34m'obscene'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            \u001b[0;34m'threat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df_results = pd.DataFrame({'id':df_test.id,\n",
    "                            'toxic':preds[0],\n",
    "                           'severe_toxic':preds[1],\n",
    "                           'obscene':preds[2],\n",
    "                           'threat':preds[3],\n",
    "                           'insult':preds[4],\n",
    "                           'identity_hate':preds[5]}).set_index('id')\n",
    "\n",
    "# Pandas automatically sorts the columns alphabetically by column name.\n",
    "# Therefore, we need to re-order the columns to match the sample submission file.\n",
    "df_results = df_results[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "\n",
    "# create a submission csv file\n",
    "df_results.to_csv('kaggle_submission.csv', \n",
    "                  columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
